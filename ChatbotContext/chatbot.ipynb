{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6144,
     "status": "ok",
     "timestamp": 1677387531291,
     "user": {
      "displayName": "Satish Sahu",
      "userId": "00112273144191307121"
     },
     "user_tz": -330
    },
    "id": "SEj0wPz2tODG",
    "outputId": "8c3e231f-3872-4ebf-c15f-3062fad25744"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# For NLP\n",
    "import nltk\n",
    "# natural language toolkit for human response understanding\n",
    "nltk.download('punkt') #for tokenization of sentences into words\n",
    "from nltk.stem.lancaster import LancasterStemmer  #used in reducing the different form of words into single base word\n",
    "#like cook, cooked, cooking into base/stem word: cook\n",
    "stemmer=LancasterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7167,
     "status": "ok",
     "timestamp": 1677387538452,
     "user": {
      "displayName": "Satish Sahu",
      "userId": "00112273144191307121"
     },
     "user_tz": -330
    },
    "id": "UW-Wva74t5_r",
    "outputId": "2c7dcf8d-9abb-4ec0-e718-e04b99f894fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tflearn\n",
      "  Using cached tflearn-0.5.0-py3-none-any.whl\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\lib\\site-packages (from tflearn) (1.21.5)\n",
      "Requirement already satisfied: Pillow in c:\\users\\user\\anaconda3\\lib\\site-packages (from tflearn) (9.2.0)\n",
      "Requirement already satisfied: six in c:\\users\\user\\anaconda3\\lib\\site-packages (from tflearn) (1.16.0)\n",
      "Installing collected packages: tflearn\n",
      "Successfully installed tflearn-0.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tflearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2860,
     "status": "ok",
     "timestamp": 1677387541308,
     "user": {
      "displayName": "Satish Sahu",
      "userId": "00112273144191307121"
     },
     "user_tz": -330
    },
    "id": "6QRj_LVVt51N",
    "outputId": "988b3f77-d76f-4e94-8c2c-b9f13f27a266"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "# Tensorflow processing\n",
    "import tensorflow as tf\n",
    "import numpy as np  \n",
    "import tflearn #for different layers in deep neural netwrok\n",
    "import random #to generate the numbers\n",
    "import json #to read the intent json file and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "executionInfo": {
     "elapsed": 102635,
     "status": "ok",
     "timestamp": 1677387643937,
     "user": {
      "displayName": "Satish Sahu",
      "userId": "00112273144191307121"
     },
     "user_tz": -330
    },
    "id": "FHMdyKtkt6Cj",
    "outputId": "f45106a3-1fd0-443a-906e-c73c9bf3ffe8"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6860\\1613494533.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1677387643937,
     "user": {
      "displayName": "Satish Sahu",
      "userId": "00112273144191307121"
     },
     "user_tz": -330
    },
    "id": "RRsm8ettt6FG"
   },
   "outputs": [],
   "source": [
    "# importing the chatbot intents\n",
    "with open('intents.json') as json_data:\n",
    "  intents=json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1677387643937,
     "user": {
      "displayName": "Satish Sahu",
      "userId": "00112273144191307121"
     },
     "user_tz": -330
    },
    "id": "H95GUNgxt6H5",
    "outputId": "ad710b4a-a8ab-4094-a8a4-f309caae6453"
   },
   "outputs": [],
   "source": [
    "intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1677387643938,
     "user": {
      "displayName": "Satish Sahu",
      "userId": "00112273144191307121"
     },
     "user_tz": -330
    },
    "id": "y0GKK8tzt6Kn"
   },
   "outputs": [],
   "source": [
    "words=[] #storing individual words\n",
    "classes=[] #storing the tags/classes of intent\n",
    "documents=[] \n",
    "ignore=['?','!'] #punctuation/speacial marks to be excluded from words\n",
    "\n",
    "for intent in intents['intents']:\n",
    "  for pattern in intent['patterns']:\n",
    "    # tokenization of each word in every sentence/pattern of patterns\n",
    "    w=nltk.word_tokenize(pattern)\n",
    "    words.extend(w) #adding individual words in empty list \n",
    "    documents.append((w,intent['tag'])) #adding words in documents for each tags\n",
    "    # adding each tags in classes list\n",
    "    if intent['tag'] not in classes:\n",
    "      classes.append(intent['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1677387643938,
     "user": {
      "displayName": "Satish Sahu",
      "userId": "00112273144191307121"
     },
     "user_tz": -330
    },
    "id": "Fea4-C_et6Mn"
   },
   "outputs": [],
   "source": [
    "# converting into lowercase and removing duplicates and stemming of words\n",
    "words=[stemmer.stem(w.lower()) for w in words if w not in ignore] #converts in lowercase+stemming+ignoring the ignore words\n",
    "words=sorted(list(set(words))) #removing the duplicate words by set operation & storing alphabetically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1677387643938,
     "user": {
      "displayName": "Satish Sahu",
      "userId": "00112273144191307121"
     },
     "user_tz": -330
    },
    "id": "sA3qIVgCt6PU",
    "outputId": "08ff534e-3c48-4c84-ab18-aa0619bccc7e"
   },
   "outputs": [],
   "source": [
    "# removing duplicate classes\n",
    "classes=sorted(list(set(classes)))\n",
    "\n",
    "print(len(documents),\"documents\")\n",
    "print(len(classes),\"classses\",classes)\n",
    "print(len(words),\"unique stemmed words\",words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1677387643938,
     "user": {
      "displayName": "Satish Sahu",
      "userId": "00112273144191307121"
     },
     "user_tz": -330
    },
    "id": "1HhWshb0t6SE",
    "outputId": "e76e230b-25c7-4d60-f64d-3e6a8a321d87"
   },
   "outputs": [],
   "source": [
    "# creating the training data set\n",
    "training=[]\n",
    "output=[]\n",
    "# create empty array for output\n",
    "output_empty=[0]*len(classes)\n",
    "\n",
    "# creating training set, bag of words for each sentence\n",
    "for doc in documents:\n",
    "  # for bag of words array\n",
    "  bag=[]\n",
    "  # list of tokenized words\n",
    "  pattern_words=doc[0]\n",
    "  # stemming each word\n",
    "  pattern_words=[stemmer.stem(word.lower()) for word in pattern_words]\n",
    "  # for current tag append 1 and for rest append 0 below \n",
    "  # by matching the patternword with words array\n",
    "  for w in words:\n",
    "    bag.append(1) if w in pattern_words else bag.append(0)\n",
    "  \n",
    "  # for current tag output is 1 and 0 for rest other tags\n",
    "  output_row=list(output_empty)\n",
    "  output_row[classes.index(doc[1])]=1\n",
    "\n",
    "  training.append([bag,output_row])\n",
    "\n",
    "# shuffling features and turning into np.array\n",
    "random.shuffle(training)\n",
    "training=np.array(training)\n",
    "\n",
    "# creating training lists\n",
    "# features(intent patterns)\n",
    "trainx=list(training[:,0])\n",
    "# labels(associated tags or labelled class)\n",
    "trainy=list(training[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 158661,
     "status": "ok",
     "timestamp": 1677387802593,
     "user": {
      "displayName": "Satish Sahu",
      "userId": "00112273144191307121"
     },
     "user_tz": -330
    },
    "id": "LUnrsyO-t6Ut",
    "outputId": "7c3e62e0-57f0-4726-b7a2-792990a427c7"
   },
   "outputs": [],
   "source": [
    "# restting underlying graph data\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "# building the neural network\n",
    "# input layer\n",
    "net=tflearn.input_data(shape=[None,len(trainx[0])])\n",
    "# 2 hidden layers\n",
    "net=tflearn.fully_connected(net,10)\n",
    "net=tflearn.fully_connected(net,10)\n",
    "# ouptut layer having trainy[0] number of fetaures\n",
    "net=tflearn.fully_connected(net,len(trainy[0]),activation=\"softmax\")\n",
    "# applying the regression on the input\n",
    "net=tflearn.regression(net)\n",
    "\n",
    "\n",
    "# dnn model\n",
    "model=tflearn.DNN(net,tensorboard_dir='tflearn_logs')\n",
    "\n",
    "# start training\n",
    "# more number of epochs more accuarcy, show_metric for showing the performancereprot\n",
    "model.fit(trainx,trainy,n_epoch=1000,batch_size=8,show_metric=True)\n",
    "# saving the model so that no need to train again and again\n",
    "# but if new intents are provided then we have to retrain the model\n",
    "model.save('model.tflearn')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1677387802594,
     "user": {
      "displayName": "Satish Sahu",
      "userId": "00112273144191307121"
     },
     "user_tz": -330
    },
    "id": "Gh_iB5Gct6Xe"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "#saving the model and intermediate data structrue\n",
    "pickle.dump({'words':words,'classes':classes,'trainx':trainx,'trainy':trainy},open(\"training_data\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "executionInfo": {
     "elapsed": 429,
     "status": "error",
     "timestamp": 1677388065653,
     "user": {
      "displayName": "Satish Sahu",
      "userId": "00112273144191307121"
     },
     "user_tz": -330
    },
    "id": "rf__2RYtHjVf",
    "outputId": "258e2067-fa95-447d-a347-2c6e37f403c8"
   },
   "outputs": [],
   "source": [
    "pickle.dump(model,open(\"trainedmodel.pkl\",'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1677387802595,
     "user": {
      "displayName": "Satish Sahu",
      "userId": "00112273144191307121"
     },
     "user_tz": -330
    },
    "id": "p_0myCmit6aM"
   },
   "outputs": [],
   "source": [
    "# restoring the all data structrues\n",
    "data=pickle.load(open('training_data','rb'))\n",
    "words=data['words']\n",
    "classes=data['classes']\n",
    "trainx=data['trainx']\n",
    "trainy=data['trainy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1677387802595,
     "user": {
      "displayName": "Satish Sahu",
      "userId": "00112273144191307121"
     },
     "user_tz": -330
    },
    "id": "kTkkPcJdt6cv"
   },
   "outputs": [],
   "source": [
    "with open('intents.json') as json_data:\n",
    "  intents=json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1677387802595,
     "user": {
      "displayName": "Satish Sahu",
      "userId": "00112273144191307121"
     },
     "user_tz": -330
    },
    "id": "-l7dNpWot6fe"
   },
   "outputs": [],
   "source": [
    "# loading the saved model\n",
    "model.load('./model.tflearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1677387802595,
     "user": {
      "displayName": "Satish Sahu",
      "userId": "00112273144191307121"
     },
     "user_tz": -330
    },
    "id": "1l1Nys4Vt6ip"
   },
   "outputs": [],
   "source": [
    "# we will take inputs from user\n",
    "# preprocess it into bag of words\n",
    "def clean_sentences(sentence):\n",
    "  # tokening the pattern\n",
    "  sentence_words=nltk.word_tokenize(sentence)\n",
    "  # stemming each word\n",
    "  sentence_words=[stemmer.stem(word.lower()) for word in sentence_words]\n",
    "  return sentence_words\n",
    "\n",
    "\n",
    "# returning the bag of words array as 0/1 for each word in bag\n",
    "def bow(sentence,words,show_details=False):\n",
    "  # tokeninz the pattern\n",
    "  sentence_words=clean_sentences(sentence)\n",
    "  # generating bag of words\n",
    "  bag=[0]*len(words)\n",
    "  for s in sentence_words:\n",
    "    for i,w in enumerate(words):\n",
    "      if w==s:\n",
    "        bag[i]=1\n",
    "        if show_details:\n",
    "          print(\"found in abd: %s\",w)\n",
    "  return (np.array(bag))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yG2rrm_zJBwx"
   },
   "source": [
    "**Response** **Processor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1677387802595,
     "user": {
      "displayName": "Satish Sahu",
      "userId": "00112273144191307121"
     },
     "user_tz": -330
    },
    "id": "sBNGIb1OJApZ"
   },
   "outputs": [],
   "source": [
    "# WITHOUT CONTEXT\n",
    "\n",
    "Error_threshold=0.30\n",
    "def classify(sentence):\n",
    "  # generate probabilities from model\n",
    "  results=model.predict([bow(sentence,words)])[0]\n",
    "  # filter out rpedictions below a threshold\n",
    "  results=[[i,r] for i,r in enumerate(results) if r>Error_threshold]\n",
    "  # sorting the strength of probabilties\n",
    "  results.sort(key=lambda x:x[1],reverse=True)\n",
    "  returnlist=[]\n",
    "  for r in results:\n",
    "    returnlist.append((classes[r[0]],r[1]))\n",
    "  # return tuple of intent and probability \n",
    "  return returnlist\n",
    "\n",
    "\n",
    "# here userID is the context about which conversations\n",
    "def response(sentence, userID='123',show_details=False):\n",
    "  results=classify(sentence)\n",
    "  # if we hahve a classification then find the best mathcng intent tag\n",
    "  if results:\n",
    "    # loop as long as there are matches to process\n",
    "    while results:\n",
    "      for i in intents['intents']:\n",
    "        # find  a tag matching the first result\n",
    "        if i['tag']==results[0][0]:\n",
    "          # a random respose from the intent\n",
    "          return print(random.choice(i['responses']))\n",
    "      results.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1677387802596,
     "user": {
      "displayName": "Satish Sahu",
      "userId": "00112273144191307121"
     },
     "user_tz": -330
    },
    "id": "OZuRFvOVJAsh"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classify' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6860\\3318657904.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# lets ask questions from chatbot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Good Morning'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'classify' is not defined"
     ]
    }
   ],
   "source": [
    "# lets ask questions from chatbot\n",
    "classify('Good Morning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6860\\609693523.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Bye'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'response' is not defined"
     ]
    }
   ],
   "source": [
    "response('Bye')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1677387802596,
     "user": {
      "displayName": "Satish Sahu",
      "userId": "00112273144191307121"
     },
     "user_tz": -330
    },
    "id": "hRl8zJ46JAwB"
   },
   "outputs": [],
   "source": [
    "# # WITH CONTEXT\n",
    "\n",
    "# context={}\n",
    "# Error_threshold=0.30\n",
    "# def classify(sentence):\n",
    "#   # generate probabilities from model\n",
    "#   results=model.predict([bow(sentence,words)])[0]\n",
    "#   # filter out rpedictions below a threshold\n",
    "#   results=[[i,r] for i,r in enumerate(results) if r>Error_threshold]\n",
    "#   # sorting the strength of probabilties\n",
    "#   results.sort(key=lambda x:x[1],reverse=True)\n",
    "#   returnlist=[]\n",
    "#   for r in results:\n",
    "#     returnlist.append((classes[r[0]],r[1]))\n",
    "#   # return tuple of intent and probability \n",
    "#   return returnlist\n",
    "\n",
    "\n",
    "# # here userID is the context about which conversations\n",
    "# def response(sentence, userID='123',show_details=False):\n",
    "#   results=classify(sentence)\n",
    "#   # if we hahve a classification then find the best mathcng intent tag\n",
    "#   if results:\n",
    "#     # loop as long as there are matches to process\n",
    "#     while results:\n",
    "#       for i in intents['intents']:\n",
    "#         # find  a tag matching the first result\n",
    "#         if i['tag']==results[0][0]:\n",
    "#           #set context for this intent if necessary\n",
    "#           if 'context_set' in i:\n",
    "#             if show_details:print('context:',i['context_set'])\n",
    "#             context[userID]=i['context_set']\n",
    "          \n",
    "#           # check if this intent is contextual and applies to users conversation\n",
    "#           if not 'context_filter' in i or \\\n",
    "#           (userID in context and 'context_filter' in i and i['context_filter']==context):\n",
    "#             if show_details:print('tag:',i['tag'])\n",
    "#             # a random respose from the intent\n",
    "#             return print(random.choice(i['responses']))\n",
    "#       results.pop(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1677387802596,
     "user": {
      "displayName": "Satish Sahu",
      "userId": "00112273144191307121"
     },
     "user_tz": -330
    },
    "id": "P0Sd8KAIJAy8",
    "outputId": "19ead8f6-865b-43e6-ec40-d3b8ce4c25b6"
   },
   "outputs": [],
   "source": [
    "# response(\"bye\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1677387802596,
     "user": {
      "displayName": "Satish Sahu",
      "userId": "00112273144191307121"
     },
     "user_tz": -330
    },
    "id": "rgJmu8opJA1Z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1677387802597,
     "user": {
      "displayName": "Satish Sahu",
      "userId": "00112273144191307121"
     },
     "user_tz": -330
    },
    "id": "wv8jCYfKJA37"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1677387802597,
     "user": {
      "displayName": "Satish Sahu",
      "userId": "00112273144191307121"
     },
     "user_tz": -330
    },
    "id": "7-fw9zN-JA6Y"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1677387802597,
     "user": {
      "displayName": "Satish Sahu",
      "userId": "00112273144191307121"
     },
     "user_tz": -330
    },
    "id": "IEMXVH8Ut6nu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1677387802597,
     "user": {
      "displayName": "Satish Sahu",
      "userId": "00112273144191307121"
     },
     "user_tz": -330
    },
    "id": "2gj2B9Hjt6qW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1677387802597,
     "user": {
      "displayName": "Satish Sahu",
      "userId": "00112273144191307121"
     },
     "user_tz": -330
    },
    "id": "T6NFeibqt6te"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMFX2pSzAsyqtEe8bmeVFg4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
